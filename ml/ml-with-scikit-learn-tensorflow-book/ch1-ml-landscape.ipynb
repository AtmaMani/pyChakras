{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning landscape\n",
    "\n",
    "## Types of ML systems\n",
    "You can classify the algorithms using different methods\n",
    " - whether or not trained by humans [`supervised, unsupervised, semi-supervised, reinforcement learning`]\n",
    " - whether or not they can learn incrementally on the fly [`online vs batch`]\n",
    " - whether they work by comparing test against train or by detecting patterns in train to predict the test data [`instance-based vs model-based`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on training required\n",
    "### Supervised learning\n",
    "You feed labeled data to the algorithm. `Classification` and `Regression` are the kinds of problems that can be solved with supervised learning. Some popular algorithms\n",
    " - k-Nearest Neighbors (KNN)\n",
    " - Linear regression\n",
    " - Logistic regression\n",
    " - Support Vector Machines (SVM)\n",
    " - Decision Trees and Random Forests\n",
    " - Neural networks\n",
    " \n",
    "### Unsupervised learning\n",
    "Training data is unlabeled. System tries to figure out the relationships. `Clustering`, `anomaly detection` and `Dimensionality Reduction` are good problems that can be solved with this type of learning. Some popular algorithms\n",
    "\n",
    " - Clustering\n",
    "   - K-Means\n",
    "   - Hierarchical Cluster Analysis (HCA)\n",
    "   - Expectation Maximization\n",
    " - Viz and dimensionality reduction\n",
    "   - Principal Component Analysis (PCA)\n",
    "   - Kernel PCA\n",
    "   - Locally-Linear Embedding (LLE)\n",
    "   - t-distributed stochastic neighbor embedding (t-SNE)\n",
    " - Association rule learning (dig into large amounts of data, find interesting relationships b/w attributes)\n",
    "   - Apriori\n",
    "   - Eclat\n",
    "\n",
    "### Semisupervised learning\n",
    "Algorithms that can learn with partially labelled data and lots of unlabeled data. Some examples of algorithms\n",
    "\n",
    " - deep belief networks (DBN)\n",
    " - restricted boltzmann machines (RBMs)\n",
    "\n",
    "### Reinforcement learning\n",
    "Learning system (`agent`) can observe the environment, select and perform actions and get `rewards` or `penalties`. It must learn by itself to get the most reward over time (`policy`). Thus a `policy` defines what action the `agent` must take in a given situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on learning rate\n",
    "### Batch learning\n",
    " - system is incapable of learning incrementally. Since training takes a lot of time and resources, it is done offline. Hence also called *offline learning*\n",
    " - when new data arrives, the system must be taken offline and trained on full dataset (not just the new part).\n",
    "\n",
    "### Online learning\n",
    " - system can be trained incrementally. Usually data is fed in mini batches.\n",
    " - this also helps if the training data is huge that it will not fit in one machine's memory. Then data can be fed in mini-batches, removed for the next set etc.\n",
    " - **learning rate** determines how fast the system can adapt to new or changing data.\n",
    " \n",
    "## Classification based on generalization\n",
    "### Instance based learning\n",
    " - learns from examples by heart then generalizes to new cases using a *measure of similarity*\n",
    "\n",
    "### Model based learning\n",
    " - system builds a model using the training data and uses the model to make predictions.\n",
    " \n",
    "## Main challenges of ML\n",
    " - insufficient training data - the \"unreasonable effectiveness of data\" paper\n",
    " - non representative training data - sampling bias, poor quality data, irrelevant features (can be rectified through feature engineering - feature selection and extraction)\n",
    " - over-fitting the training data - happens when the model is too complex relative to the amount of noisiness of the data. One solution is to *constrain* the model.\n",
    "   - **hyperparameters** - \n",
    " - underfitting the training data - when the model is too simple to learn the phenomena in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
